% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nmf.R
\name{nmf}
\alias{nmf}
\title{Alternating matrix factorization}
\usage{
nmf(
  A,
  k,
  min = 0,
  max = NULL,
  L0 = NULL,
  L1 = 0,
  L2 = 0,
  PE = 0,
  exact = FALSE,
  cd = FALSE,
  trace = 1,
  maxit = 100,
  maxit_cd = 100,
  tol_cd = 1e-08,
  diag = TRUE,
  seed = NULL,
  threads = 0,
  verbose = TRUE,
  full_path = FALSE,
  tol_wh = 0.001,
  loss = NULL,
  tol_loss = 0.001,
  ...
)
}
\arguments{
\item{A}{matrix of features (rows) by samples (columns) to be factorized. Provide in dense
or sparse format.}

\item{k}{rank of factorization}

\item{min}{lower bound on solution, usually 0 (default). Vector of two for \code{c(w, h)}}

\item{max}{upper bound on solution, if applicable (NULL, default). Vector of two for
\code{c(w, h)}.}

\item{L0}{cardinality of least squares solutions, if other than full-rank (NULL, default). If 1,
an efficient exact solver is used. If 2 or greater, all possible feasible sets are sampled using
exact NNLS (NP-hard). Specify with caution, see details. Vector of two for \code{c(w, h)}}

\item{L1}{LASSO penalty to be subtracted from right-hand side of coefficients in least squares
updates, scaled to the maximum value in the right-hand side of each least squares problem.
Vector of two for \code{c(w, h)}.}

\item{L2}{Ridge regression penalty to be added to diagonal of \eqn{a}. Scaled to mean diagonal
value in coefficient matrix. Vector of two for \code{c(w, h)}.}

\item{PE}{Pattern extraction penalty to be added to off-diagonal values in least squares
updates, scaled to mean diagonal value of the coefficient matrix. Vector of two for \code{c(w, h)
}.}

\item{exact}{calculate exact solution? See \code{\link{nnls}} for details. Use with caution.
Vector of two for \code{c(w, h)}.}

\item{cd}{use coordinate descent to refine the solution? Vector of two for \code{c(w, h)}}

\item{trace}{calculate applicable tolerances / losses every trace iterations, and check for
convergence}

\item{maxit}{maximum number of alternating updates of \eqn{w} and \eqn{h}}

\item{maxit_cd}{in coordinate descent, maximum number of iterations}

\item{tol_cd}{in coordinate descent, distance of solution from maximum gradient residual at
convergence}

\item{diag}{use a diagonal to scale columns in \eqn{w} and rows in \eqn{h} to sum to 1 (default
\code{TRUE})}

\item{seed}{seed for random initialization of \eqn{w}}

\item{threads}{use column-wise parallelization for each update of \eqn{w} and \eqn{h}. Specify
either 0 (all available threads, default) 1 (no parallelization), or any valid number.}

\item{verbose}{show tolerance for each iteration}

\item{full_path}{return all intermediate models in addition to the final model (default FALSE)}

\item{tol_wh}{stop factorizing when the relative change in \eqn{h} multiplied by the relative
change in \eqn{w} between consecutive iterations falls below this value.}

\item{loss}{One of c(\code{NULL}, \code{"mse"}, \code{"mae"}) to either not calculate loss
(which is computationally expensive) or to calculate mean squared or absolute error of the
factorization every \code{trace} iterations (and potentially use loss as a convergence criteria
by specifying \code{tol_loss}). NULL by default, because loss calculations are slower than the
factorization itself.}

\item{tol_loss}{stop factorizing when the relative change in loss between consecutive iterations
falls below this value. If \code{loss} is not NULL, loss tolerance will be calculated.}

\item{...}{additional parameters}

\item{tol_wh_switch_to_cd}{begin using coordinate descent solver exclusively at this tolerance,
as opposed to cold-start consisting of FAST initialization followed by coordinate descent.}
}
\value{
An \code{\link{nmfmodel}} object with slots:
\itemize{
\item \code{w}: matrix of \emph{features x factors} of dimensions \emph{m x k}
\item \code{d}: diagonal scaling vector of length \emph{k}
\item \code{h}: matrix of \emph{factors x samples} of dimensions \emph{k x n}
\item \code{loss}: mean squared/absolute error of the model (if \code{loss} is not NULL)
\item \code{tol_wh}: tolerance of change in \emph{w} multiplied by the change in \emph{h} between
the last two consecutive iterations
\item \code{tol_loss}: mean squared/absolute error of the model (if \code{loss} is not NULL)
\item \code{it}: number of iterations used to solve the model
\item \code{path}: a list of \code{\link{nmfmodel}} objects containing intermediate model states
along the full solution path (if \code{full_path = TRUE})
}
}
\description{
Non-negative matrix factorization, or other constrained factorization, by
alternating constrained least squares with fast solvers and efficient stopping criteria.
}
\details{
NMF solves the equation \deqn{A = WH} by minimizing \eqn{A - WH}. In NMF by alternating least
squares, this objective is implicitly minimized and is thus never explicitly calculated. Rather,
progress towards the objective is measured indirectly by the rate of change in \eqn{H} between
consecutive iterations.
\subsection{Model diagonalization}{

By default, \code{nmf} scales columns in \eqn{w} and rows in \eqn{h} to sum to 1 by
introducing a "scaling diagonal":
\deqn{A = WDH}
This is important in several ways:
\itemize{
\item \strong{interpretability}: it is easy to see which features (or samples) are most important in a
factor, not as a proportion of the total signal explained by the model, but as a proportion of
contribution to that specific factor/signal.
\item \strong{regularization}: regularization is performed on the normalized matrices, thus regularization
in a diagonalized model penalizes signals equally and not as a proportion of their relative
contribution towards overall signal.
\item \strong{multiple model alignment}: factorizations from multiple random starts rarely align.
However, after sorting along the diagonal, they are much more likely to naturally align.
}
}

\subsection{Model regularization}{

Regularization, especially sparsifying regularization (i.e. L0, L1, PE) challenges the discovery
of the best model solution. It may be most useful to first solve the unpenalized model, and then
add regularizations one at a time.
}

\subsection{Exact solutions}{

The challenges of exact nnls have been reviewed in \code{link{nnls}}. Exact NNLS is NP-hard
because it considers every possible feasible set.
}

\subsection{NNLS solvers used}{

At present, only cold-start solvers are used (i.e. FAST, FAST + CD, nnls2, L0 = 1, and exact).
In the future, support will be added for warm-start solvers if they are faster.
}
}
\seealso{
\code{\link{nnls}}, \code{\link{nmfmodel-class}}
}
